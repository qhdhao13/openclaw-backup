#!/usr/bin/env python3
"""
è‚¡ç¥¨æ–°é—»è‡ªåŠ¨çˆ¬å–ç³»ç»Ÿ
ä»ä¸œæ–¹è´¢å¯Œã€æ–°æµªè´¢ç»çˆ¬å–å®æ—¶æ–°é—»
"""
import sys
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import json
import re
import time

class StockNewsCrawler:
    """è‚¡ç¥¨æ–°é—»çˆ¬è™«"""
    
    def __init__(self, stock_code, stock_name=None):
        self.stock_code = stock_code
        self.stock_name = stock_name or stock_code
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
    
    def crawl_eastmoney(self, days=3):
        """çˆ¬å–ä¸œæ–¹è´¢å¯Œæ–°é—»"""
        news_list = []
        try:
            # ä¸œæ–¹è´¢å¯Œä¸ªè‚¡æ–°é—»é¡µé¢
            url = f"https://quote.eastmoney.com/concept/sh{self.stock_code}.html"
            # å¤‡ç”¨URL
            url2 = f"https://emweb.securities.eastmoney.com/PC_HSF10/NewStockAnalysis/Index?type=web&code=SZ{self.stock_code}"
            
            # å°è¯•è·å–æ–°é—»
            try:
                response = requests.get(url2, headers=self.headers, timeout=10)
                response.encoding = 'utf-8'
            except:
                return news_list
            
            # è§£ææ–°é—»ï¼ˆä¸œæ–¹è´¢å¯Œé¡µé¢ç»“æ„å¤æ‚ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å°è¯•æå–æ–°é—»æ ‡é¢˜
            news_items = soup.find_all('a', href=re.compile(r'news'))
            for item in news_items[:10]:  # å–å‰10æ¡
                title = item.get_text().strip()
                if title and len(title) > 10:
                    news_list.append({
                        "title": title,
                        "source": "ä¸œæ–¹è´¢å¯Œ",
                        "time": datetime.now().strftime("%Y-%m-%d"),
                        "url": item.get('href', '')
                    })
            
            return news_list
        except Exception as e:
            print(f"ä¸œæ–¹è´¢å¯Œçˆ¬å–å¤±è´¥: {e}")
            return []
    
    def crawl_sina_finance(self):
        """çˆ¬å–æ–°æµªè´¢ç»æ–°é—»"""
        news_list = []
        try:
            # æ–°æµªè´¢ç»ä¸ªè‚¡é¡µé¢
            url = f"https://finance.sina.com.cn/realstock/company/sz{self.stock_code}/nc.shtml"
            
            response = requests.get(url, headers=self.headers, timeout=10)
            response.encoding = 'gb2312'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ–°æµªæ–°é—»åˆ—è¡¨é€šå¸¸åœ¨ç‰¹å®šclassä¸­
            news_items = soup.find_all('a', target='_blank')
            
            for item in news_items[:15]:
                title = item.get_text().strip()
                href = item.get('href', '')
                
                # è¿‡æ»¤æœ‰æ•ˆæ–°é—»
                if (title and len(title) > 10 and 
                    'finance.sina.com.cn' in href and
                    any(keyword in title for keyword in ['ä¸šç»©', 'è¥æ”¶', 'åˆ©æ¶¦', 'å…¬å‘Š', 'è®¢å•', 'é¡¹ç›®', 'æŠ•èµ„', 'åˆä½œ'])):
                    
                    news_list.append({
                        "title": title,
                        "source": "æ–°æµªè´¢ç»",
                        "time": datetime.now().strftime("%Y-%m-%d"),
                        "url": href
                    })
            
            return news_list
        except Exception as e:
            print(f"æ–°æµªè´¢ç»çˆ¬å–å¤±è´¥: {e}")
            return []
    
    def crawl_10jqka(self):
        """çˆ¬å–åŒèŠ±é¡ºæ–°é—»"""
        news_list = []
        try:
            url = f"https://basic.10jqka.com.cn/{self.stock_code}/news.html"
            
            response = requests.get(url, headers=self.headers, timeout=10)
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»åˆ—è¡¨
            news_items = soup.find_all('a', class_=re.compile(r'news|title'))
            
            for item in news_items[:10]:
                title = item.get_text().strip()
                if title and len(title) > 10:
                    news_list.append({
                        "title": title,
                        "source": "åŒèŠ±é¡º",
                        "time": datetime.now().strftime("%Y-%m-%d"),
                        "url": item.get('href', '')
                    })
            
            return news_list
        except Exception as e:
            print(f"åŒèŠ±é¡ºçˆ¬å–å¤±è´¥: {e}")
            return []
    
    def crawl_all(self):
        """çˆ¬å–æ‰€æœ‰æ¥æºçš„æ–°é—»"""
        print(f"æ­£åœ¨çˆ¬å– {self.stock_name}({self.stock_code}) çš„æ–°é—»...\n")
        
        all_news = []
        
        # çˆ¬å–å„å¹³å°
        sources = [
            ("ä¸œæ–¹è´¢å¯Œ", self.crawl_eastmoney),
            ("æ–°æµªè´¢ç»", self.crawl_sina_finance),
            ("åŒèŠ±é¡º", self.crawl_10jqka)
        ]
        
        for source_name, crawler_func in sources:
            try:
                print(f"æ­£åœ¨çˆ¬å– {source_name}...")
                news = crawler_func()
                all_news.extend(news)
                print(f"âœ… {source_name}: è·å– {len(news)} æ¡")
                time.sleep(1)  # ç¤¼è²Œçˆ¬å–
            except Exception as e:
                print(f"âŒ {source_name}: {e}")
        
        # å»é‡ï¼ˆåŸºäºæ ‡é¢˜ç›¸ä¼¼åº¦ï¼‰
        unique_news = self._deduplicate_news(all_news)
        
        return unique_news
    
    def _deduplicate_news(self, news_list):
        """å»é™¤é‡å¤æ–°é—»"""
        seen_titles = set()
        unique = []
        
        for news in news_list:
            # ç®€åŒ–æ ‡é¢˜ç”¨äºå»é‡
            simplified = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', '', news['title'])
            if simplified not in seen_titles and len(simplified) > 5:
                seen_titles.add(simplified)
                unique.append(news)
        
        return unique

def analyze_news_sentiment_local(news_list):
    """æœ¬åœ°ç®€å•æƒ…æ„Ÿåˆ†æ"""
    positive_words = ['å¢é•¿', 'ä¸Šæ¶¨', 'çªç ´', 'åˆ©å¥½', 'ç›ˆåˆ©', 'å¢æŒ', 'ä¹°å…¥', 'çœ‹å¥½', 'è®¢å•', 'åˆä½œ', 'ç­¾çº¦']
    negative_words = ['ä¸‹è·Œ', 'äºæŸ', 'å‡æŒ', 'å–å‡º', 'é£é™©', 'è­¦å‘Š', 'å¤„ç½š', 'ä¸‹æ»‘', 'ä¸‹é™']
    
    results = []
    for news in news_list:
        title = news['title']
        pos_count = sum(1 for word in positive_words if word in title)
        neg_count = sum(1 for word in negative_words if word in title)
        
        if pos_count > neg_count:
            sentiment = "æ­£é¢"
        elif neg_count > pos_count:
            sentiment = "è´Ÿé¢"
        else:
            sentiment = "ä¸­æ€§"
        
        news['sentiment'] = sentiment
        results.append(news)
    
    return results

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("""
è‚¡ç¥¨æ–°é—»çˆ¬å–ç³»ç»Ÿ

ç”¨æ³•:
  python3 stock_news.py 000338 æ½æŸ´åŠ¨åŠ›
        """)
        return
    
    stock_code = sys.argv[1]
    stock_name = sys.argv[2] if len(sys.argv) > 2 else stock_code
    
    # çˆ¬å–æ–°é—»
    crawler = StockNewsCrawler(stock_code, stock_name)
    news_list = crawler.crawl_all()
    
    if not news_list:
        print("\nâš ï¸ æœªè·å–åˆ°æ–°é—»æ•°æ®")
        return
    
    # æƒ…æ„Ÿåˆ†æ
    analyzed_news = analyze_news_sentiment_local(news_list)
    
    # ç»Ÿè®¡
    positive = sum(1 for n in analyzed_news if n['sentiment'] == 'æ­£é¢')
    negative = sum(1 for n in analyzed_news if n['sentiment'] == 'è´Ÿé¢')
    neutral = sum(1 for n in analyzed_news if n['sentiment'] == 'ä¸­æ€§')
    
    # è¾“å‡ºç»“æœ
    print(f"\n=== {stock_name}({stock_code}) æ–°é—»åˆ†æç»“æœ ===\n")
    print(f"æ€»è®¡è·å–: {len(analyzed_news)} æ¡æ–°é—»")
    print(f"æƒ…æ„Ÿåˆ†å¸ƒ: æ­£é¢ {positive} | è´Ÿé¢ {negative} | ä¸­æ€§ {neutral}\n")
    
    print("ğŸ“° æœ€æ–°æ–°é—»ï¼ˆTop 10ï¼‰ï¼š")
    print("-" * 80)
    
    for i, news in enumerate(analyzed_news[:10], 1):
        emoji = "ğŸŸ¢" if news['sentiment'] == 'æ­£é¢' else "ğŸ”´" if news['sentiment'] == 'è´Ÿé¢' else "âšª"
        print(f"{i}. {emoji} [{news['source']}] {news['title']}")
    
    # ä¿å­˜ç»“æœ
    output_file = f"news_{stock_code}_{datetime.now().strftime('%Y%m%d')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(analyzed_news, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")

if __name__ == "__main__":
    main()
